1) Data-level methods
- see Advanced ML (goes beyond lecture)
- Undersampling: Tomek links, Condensed nearest neighbours
- Oversampling: SMOTE

2) Algorithmic methods
- Balanced loss

3) Hybrid methods
- Possibly combine both methods

4) Optimization
- Use Bayesian optimization
- Use DEHB, but multiple fidelities?
- Maybe use SMAC or choose model where longer runtimes improve model.

Ideas:
- Check imputation, e.g. mean, median, mode, sample from edf, random forest imputer.
- Replace categorical features by new level.
- Consider feature engineering, e.g. skewed distributions, normalization, standardization, etc.; Use scaling in sklearn, see pg. 146.
- Check for duplicated columns or constant columns.
- Add imbalanced data methods.
- Read about DEHB: Do I need restarts, since HB already initializes multiple defaults?
- Check for paralellization of algorithm.
- Check for hierarchical search spaces.
- Check out Auto-WEKA if we want to use algorithm selection. Issue: Written in Java.
- Check whether actual cost and budget corresponds. 

ToDo:
- Save CV correctly
- Write docstrings.
- Improve code quality to be more robust.
- Check whether multiple fidelities make sense with RF? Mabye we should use other algorithm.


Visualisation
- Plot incumbent accuracy over number.
- Plot sampling process of hyperparameter, either by picking two hyperparameter or by downscaling with MDS.
- Parallel coordinate plot.

Comments
- Outer resampling should be more precise since actual generalization performance, inner resampling can be less precise.

