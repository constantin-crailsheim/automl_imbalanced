\documentclass[11pt]{article}

% This file will be kept up-to-date at the following GitHub repository:
%
% https://github.com/automl-conf/LatexTemplate
%
% Please file any issues/bug reports, etc. you may have at:
%
% https://github.com/automl-conf/LatexTemplate/issues

\usepackage{microtype} % microtypography
\usepackage{booktabs}  % tables
\usepackage{url}  % urls

% AMS math
\usepackage{amsmath}
\usepackage{amsthm}

% With no package options, the submission will be anonymized, the supplemental
% material will be suppressed, and line numbers will be added to the manuscript.
%
% To hide the supplementary material (e.g., for the first submission deadline),
% use the [hidesupplement] option:
%
% \usepackage[hidesupplement]{automl}
%
% To compile a non-anonymized camera-ready version, add the [final] option (for
% the main track), or the [finalworkshop] option (for the workshop track), e.g.,
%
% \usepackage[final]{automl}
% \usepackage[finalworkshop]{automl}
%
% or
%
% \usepackage[final, hidesupplement]{automl}
% \usepackage[finalworkshop, hidesupplement]{automl}

\usepackage[final]{automl}

% You may use any reference style as long as you are consistent throughout the
% document. As a default we suggest author--year citations; for bibtex and
% natbib you may use:

\usepackage{natbib}
\bibliographystyle{apalike}

% and for biber and biblatex you may use:

% \usepackage[%
%   backend=biber,
%   style=authoryear-comp,
%   sortcites=true,
%   natbib=true,
%   giveninits=true,
%   maxcitenames=2,
%   doi=false,
%   url=true,
%   isbn=false,
%   dashed=false
% ]{biblatex}
% \addbibresource{...}

% Own packages
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\title{Example Submission Final Project AutoML Lecture WS 2022/2023}

% The syntax for adding an author is
%
% \author[i]{\nameemail{author name}{author email}}
%
% where i is an affiliation counter. Authors may have
% multiple affiliations; e.g.:
%
% \author[1,2]{\nameemail{Anonymous}{anonymous@example.com}}

\author[1]{\nameemail{Constantin von Crailsheim}{C.Crailsheim@campus.lmu.de}}

% the list might continue:
% \author[2,3]{\nameemail{Author 2}{email2@example.com}}
% \author[3]{\nameemail{Author 3}{email3@example.com}}
% \author[4]{\nameemail{Author 4}{email4@example.com}}

% if you need to force a linebreak in the author list, prepend an \author entry
% with \\:

% \author[3]{\\\nameemail{Author 5}{email5@example.com}}

% Specify corresponding affiliations after authors, referring to counter used in
% \author:

\affil[1]{LMU Munich, Institute of Statistics}

% the list might continue:
% \affil[2]{Institution 2}
% \affil[3]{Institution 3}
% \affil[4]{Institution 4}

% define PDF metadata, please fill in to aid in accessibility of the resulting PDF
\hypersetup{%
  pdfauthor={}, % will be reset to "Anonymous" unless the "final" package option is given
  pdftitle={},
  pdfsubject={},
  pdfkeywords={}
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

% content will be automatically hidden during submission
% \begin{acknowledgements}

% \end{acknowledgements}

\section{Introduction}


The AutoML system optimizes a ML pipeline consisting of four elements. First, a choice of two imputers replaces missing values. Then, either over- or undersampling was applied to deal with imbalance in the targets. After normalizing all features, three different models were fitted, i.e. a random forest classifier, a gradient boosting classifier and a SVM classifier. The choice of pre-processing and the hyperparameters of the model were optimized by DEHB. Finally, the best pipelines for each model were stacked and the final AutoML system predicts the target using majority voting.

\section{Method}

Choices of imputation strategy of sklearn.impute:
\begin{itemize}
\item The \href{https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html}{SimpleImputer} is an univariate imputer, which completes missing values with a descriptive statistic per feature. I chose the median, since it is less sensitive to outliers than the mean and will most likely still produce sensible imputed values for categorical features.
\item The \href{https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer}{KNNImputer} replaces missing values by the mean value for that feature of its 5 nearest neighbors based on Euclidian distance of their non-missing feature observations. 
\end{itemize}

Choices of data-level sampling method to yield balanced dataset of imblearn:
\begin{itemize}
\item \href{https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html}{SMOTE} as an oversampling approach generates new samples of the minority class by interpolating between existing observations of the minority class, while not distinguishing between easy and hard samples. 
\item \href{https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.TomekLinks.html}{TomekLinks} as an undersampling approach removes samples from the majority class, if they are nearest neighbors to a minority class sample, thus removing noisy borderline examples of the majority class. 
\item \href{https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTETomek.html}{SMOTETomek} which combines SMOTE and Tomek links.
\item No sampling method, which would allow algorithmic-level methods to deal with the imbalanced data. 
\end{itemize}

Since the above pre-processing methods impute missing values by means or medians and generate new samples by interpolation, values for categorical features could be generated which are not actually categorical anymore. Thus, another layer is added to the pipeline, which rounds all observations of categorical features to an integer. \\ 

The last step of pre-processing is  the choice of whether to apply the \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}{StandardScaler} of sklearn.preprocessing to standardize the features. This will be in particular useful for the SVM, since a RBF kernel assumes features centered around zero and similar variance across features. \\

Subsequently, a the hyperparameter for the three models of the stack were optimized, where the search space was defined with the \href{https://automl.github.io/ConfigSpace/main/}{ConfigSpace} package. For almost all hyperparameter, the default was set to the default of the model. The search space was mostly chosen such that it is centered around the default while accounting for log scale. In those cases where the default would be a the lower end of a reasonable search space, the upper bound was chosen higher. \\

The first model in the stack is a \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}{RandomForestClassifier} from sklearn.ensemble. It can handle all data types well and generalizes well by having a low variance due to ensembling over relatively uncorrelated models. The hyperparameters to be optimized, which will all be sampled uniform, are as following:

\begin{table}[h]
\begin{tabular}{ | c | c | c | c | c | c | }
 \hline
  Hyperparameter & Data type & Search space & Default & Other \\
 \hline
 criterion & Categorical & \{Gini, Entropy, Log loss\} & Gini &   \\ 
 max\_depth  & Integer & [5,15] & 10 &  \\ 
 min\_samples\_split & Integer & [1, 32] & 2 & Log scale \\ 
 min\_samples\_leaf & Integer & [1, 16] & 1 & Log scale  \\ 
 max\_features & Integer & [0.1, 0.9] & 0.5 &   \\  
 class\_weight & Categorical & \{Balanced, Balanced subsample, None\}  & None &  \\ 
 \hline
\end{tabular}
\end{table}

Comment on hyperparameter \\

The second model in the stack is a \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html}{GradientBoostingClassifier} from sklearn.ensemble. Why? The hyperparameters to be optimized are as following:

\begin{table}[h]
\begin{tabular}{ | c | c | c | c | c | c | }
 \hline
  Hyperparameter & Data type & Search space & Default & Other \\
 \hline
 loss & Categorical & \{Log loss, Exponential\} & Log loss &  \\ 
 learning\_rate & Float & [0.01, 1] & 0.1 & Log scale  \\ 
 criterion & Categorical & \{Friedman MSE, Squared error\} & Friedman MSE &   \\ 
 min\_samples\_split & Integer & [1, 32] & 2 & Log scale \\ 
 min\_samples\_leaf & Integer & [1, 16] & 1 & Log scale  \\
 max\_depth  & Integer & [2,10] & 3 &  \\ 
 \hline
\end{tabular}
\end{table}

Comment on hyperparameter \\

The third model in the stack is a Support Vector Classifier (\href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC}{SVC}) from sklearn.svm. Why? The hyperparameters to be optimized are as following:
\begin{table}[h]
\begin{tabular}{ | c | c | c | c | c | c | }
 \hline
  Hyperparameter & Data type & Search space & Default & Other \\
 \hline
 C & Float & [0.1, 10]  & 1.0 & Log scale \\ 
 kernel & Categorical & \{Linear, Polynomial, RBF, Sigmoid\}  & RBF &   \\ 
 shrinking & Boolan & \{True, False\} & True &  \\ 
 tol & Float & [1e-5,1e-3] & 1e-3 &  \\ 
 class\_weight & Categorical & \{Balanced, None\}  & None &  \\ 
 \hline
\end{tabular}
\end{table}

Comment on hyperparameter \\

To optimize the hyperparameter of the AutoML system, \href{https://github.com/automl/dehb}{DEHB} by \citet{dehb} was used, which combines Differential Evolution and Hyperband. Differential evolution generates a new mutant vector from three random parents and then generates the offspring by random selecting values from the new mutant vector with probability $p$ and from on of the corresponding parents otherwise. Hyperband allows to search the whole search space with cheap evaluations and only train more expensive models on promising areas of the search space. The algorithm starts by sampling $N=\eta^{f-1}$ random hyperparameter configurations, which are evaluated at the lowest budget. Then the best $1/\eta$ of the configurations are evaluated at a $\eta$ times as higher fidelity and this process is repeated until the highest fidelity (denoted here by $f$) is reached. After completing one iteration, the algorithm restarts with new instantiations, but evaluating these at the second lowest fidelity. DEHB combines both approaches by generating the configurations for the next fidelity by differential evolution from the lower fidelity as parent pool, where the previous evaluations indicate promising regions. The authors state that DEHB is computationally cheap with high speed-up gains compared to BOHB. Furthermore, it has strong final performance for discrete search spaces, which we have for various hyperparameter. Their experiments have shown that DEHB also outperforms SMAC by mean ranks across all benchmarks. For those reasons, DEHB was chosen as an efficient optimizer. Key features of DEHB: \\

For the optimization, $\eta=3$ and $f=4$, which implies an initial population of $N=3^{3}=27$. The budget for the RandomForestClassifier and the GradientBoostingClassifier, as indicated by the number of trees in the forest, was set to a minimum of 20 and maximum of 540. For SVC, the budget is indicated by the maximum number of iterations and was set to a minimum of 500 and maximum of 13500. However, the runtime between the lowest and largest budget did not differ too much and was usually around 0.2 to 1.0 seconds. Thus, the SVC mostly benefits from the differential evolution and the successive halving element is not as important since many configurations can be tested irrespecitively. The evaluations for the RandomForestClassifier and GradientBoostingClassifier, the evaluations at higher fidelities are more costly (How costly?) Thus, 40\% of the maximum cost was allocated to optimizing these models, and 20\% was allocated to optimizing the SVC. \\

To evaluate the performance of the AutoML system, 3-fold external and 4-fold internal cross-validation was used. Given a total budget of 3600 seconds per dataset, a total of 1200 second could be used to optimize the AutoML system in each fold, with 480 second allocated to the tree models and 240 seconds to the SVC. To evaluate each hyperparameter configuration, 4-fold cross-validation was used. Since the budget per hyperparameter configuration is not that big after accounting for cross-validation, only a selection of hyperparameters of the actual model were optimized and the hyperparameters of the preprocessing functions were kept at their default values. \\

After the optimization routine, each model is fitted with incumbent configuration and the unbalanced sampling is removed from the pipeline to not change the test set. The final AutoML system is stacked version of three models with majority voting for the final classification.

\section{Experiments}

The external cross-validation performance in terms of accuracy of the AutoML system vs. the untuned random forest baseline for each dataset id is shown below:

\begin{table}[h]
\begin{tabular}{lrr}
\toprule
        Model &   976 &  1002 \\
\midrule
   Baseline & 0.960 & 0.537 \\
  AutoML system & 0.985 & 0.810 \\
\midrule
  Improvement & 0.025 & 0.273 \\
\bottomrule
\end{tabular}
\end{table}

The chosen incumbents for each model for dataset x are shown below: \\




Example of trajectory for dataset x.


\section{Conclusion}

Improvements: Also optimize hyperparameter of pre-processing methods.




\bibliography{References.bib}

% supplemental material -- everything hereafter will be suppressed during
% submission time if the hidesupplement option is provided!

\appendix

\section{Trajectories}

Bla Bla

\section{Incumbent hyperparameter}

\end{document}
